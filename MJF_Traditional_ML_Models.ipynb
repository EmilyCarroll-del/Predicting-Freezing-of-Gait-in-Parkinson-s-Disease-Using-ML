{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmilyCarroll-del/Michael-J-Fox-Foundation-FOG-in-PD/blob/main/MJF_Traditional_ML_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgpp8KNa9T9t",
        "outputId": "85266c3b-f8fd-4535-ab11-2a3b49ae3a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf0nYGrz93g5",
        "outputId": "e680f67f-c24f-4709-9d63-e6e035935ab8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n",
            "'Copy of defog_start_hesitation.csv'          defog_turn_draft.csv\n",
            "'Copy of defog_start_hesitation_test.csv'     defog_turn_test_draft.csv\n",
            "'Copy of defog_turn.csv'                      defog_walk_draft.csv\n",
            "'Copy of defog_turn_test.csv'                 defog_walk_test_draft.csv\n",
            "'Copy of defog_walking.csv'                   MJFF-FOG-Prediction-PD.ipynb\n",
            "'Copy of defog_walking_test.csv'              tdcsfog_sh_draft.csv\n",
            "'Copy of tdcsfog_start_hesitation.csv'        tdcsfog_sh_test_draft.csv\n",
            "'Copy of tdcsfog_start_hesitation_test.csv'   tdcsfog_turn_draft.csv\n",
            "'Copy of tdcsfog_turn.csv'                    tdcsfog_turn_test_draft.csv\n",
            "'Copy of tdcsfog_turn_test.csv'               tdcsfog_walk_draft.csv\n",
            "'Copy of tdcsfog_walking.csv'                 tdcsfog_walk_test_draft.csv\n",
            "'Copy of tdcsfog_walking_test.csv'            \u001b[0m\u001b[01;34mTest\u001b[0m/\n",
            " defog_sh_draft.csv                           \u001b[01;34mtlvmc-parkinsons-freezing-gait-prediction\u001b[0m/\n",
            " defog_sh_test_draft.csv\n"
          ]
        }
      ],
      "source": [
        "%cd '/content/drive/MyDrive/Colab Notebooks'\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1aMDPYf-OuP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh29Dc18U4c9"
      },
      "outputs": [],
      "source": [
        "#preprocessed training data\n",
        "defog_turn = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of defog_turn.csv')\n",
        "defog_walk = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of defog_walking.csv')\n",
        "defog_sh = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of defog_start_hesitation.csv')\n",
        "\n",
        "tdcsfog_turn = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of tdcsfog_turn.csv')\n",
        "tdcsfog_walk = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of tdcsfog_walking.csv')\n",
        "tdcsfog_sh = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of tdcsfog_start_hesitation.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ynGxiEGV631"
      },
      "outputs": [],
      "source": [
        "#preprocessed test data\n",
        "defog_turn_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of defog_turn_test.csv')\n",
        "defog_walk_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of defog_walking_test.csv')\n",
        "defog_sh_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of defog_start_hesitation_test.csv')\n",
        "\n",
        "tdcsfog_turn_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of tdcsfog_turn_test.csv')\n",
        "tdcsfog_walk_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of tdcsfog_walking_test.csv')\n",
        "tdcsfog_sh_test = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Copy of tdcsfog_start_hesitation_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIYc-hP5Y4h5"
      },
      "outputs": [],
      "source": [
        "#unnamed columns\n",
        "defog_turn = defog_turn.drop(defog_turn.columns[0], axis=1)\n",
        "defog_walk = defog_walk.drop(defog_walk.columns[0], axis=1)\n",
        "defog_sh = defog_sh.drop(defog_sh.columns[0], axis=1)\n",
        "\n",
        "tdcsfog_turn = tdcsfog_turn.drop(tdcsfog_turn.columns[0], axis=1)\n",
        "tdcsfog_walk = tdcsfog_walk.drop(tdcsfog_walk.columns[0], axis=1)\n",
        "tdcsfog_sh = tdcsfog_sh.drop(tdcsfog_sh.columns[0], axis=1)\n",
        "\n",
        "defog_turn_test = defog_turn_test.drop(defog_turn_test.columns[0], axis=1)\n",
        "defog_walk_test = defog_walk_test.drop(defog_walk_test.columns[0], axis=1)\n",
        "defog_sh_test = defog_sh_test.drop(defog_sh_test.columns[0], axis=1)\n",
        "\n",
        "tdcsfog_turn_test = tdcsfog_turn_test.drop(tdcsfog_turn_test.columns[0], axis=1)\n",
        "tdcsfog_walk_test = tdcsfog_walk_test.drop(tdcsfog_walk_test.columns[0], axis=1)\n",
        "tdcsfog_sh_test = tdcsfog_sh_test.drop(tdcsfog_sh_test.columns[0], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rycDPCTFMq9p"
      },
      "outputs": [],
      "source": [
        "X_train_dturn = defog_turn.drop(columns='Turn')\n",
        "y_train_dturn = defog_turn['Turn']\n",
        "############################\n",
        "X_train_dwalk = defog_walk.drop(columns='Walking')\n",
        "y_train_dwalk = defog_walk['Walking']\n",
        "############################\n",
        "X_train_dsh = defog_sh.drop(columns='StartHesitation')\n",
        "y_train_dsh = defog_sh['StartHesitation']\n",
        "\n",
        "#--------------------------#\n",
        "\n",
        "X_train_tturn = tdcsfog_turn.drop(columns='Turn')\n",
        "y_train_tturn = tdcsfog_turn['Turn']\n",
        "############################\n",
        "X_train_twalk = tdcsfog_walk.drop(columns='Walking')\n",
        "y_train_twalk = tdcsfog_walk['Walking']\n",
        "############################\n",
        "X_train_tsh = tdcsfog_sh.drop(columns='StartHesitation')\n",
        "y_train_tsh = tdcsfog_sh['StartHesitation']\n",
        "\n",
        "#--------------------------#\n",
        "\n",
        "X_test_dturn = defog_turn_test.drop(columns=['Turn'])\n",
        "y_test_dturn = defog_turn_test['Turn']\n",
        "\n",
        "X_test_dwalk = defog_walk_test.drop(columns=['Walking'])\n",
        "y_test_dwalk = defog_walk_test['Walking']\n",
        "\n",
        "X_test_dsh = defog_sh_test.drop(columns=['StartHesitation'])\n",
        "y_test_dsh = defog_sh_test['StartHesitation']\n",
        "############################\n",
        "X_test_tturn = tdcsfog_turn_test.drop(columns=['Turn'])\n",
        "y_test_tturn = tdcsfog_turn_test['Turn']\n",
        "\n",
        "X_test_twalk = tdcsfog_walk_test.drop(columns=['Walking'])\n",
        "y_test_twalk = tdcsfog_walk_test['Walking']\n",
        "\n",
        "X_test_tsh = tdcsfog_sh_test.drop(columns=['StartHesitation'])\n",
        "y_test_tsh = tdcsfog_sh_test['StartHesitation']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz7pRa4OVYOr"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "models = {\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM__PyGkbEjw"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Hyperparameter grids for each model\n",
        "param_grids = {\n",
        "    \"KNN\": {\n",
        "        \"n_neighbors\": [3, 5, 7, 9],\n",
        "        \"weights\": [\"uniform\", \"distance\"],\n",
        "        \"metric\": [\"euclidean\", \"manhattan\", \"minkowski\"]  # Added distance metrics\n",
        "    },\n",
        "    \"Logistic Regression\": {\n",
        "        \"C\": [0.01, 0.1, 1, 10],\n",
        "        \"penalty\": [\"l2\", \"l1\"],  # Added 'l1' penalty for sparsity\n",
        "        \"solver\": [\"lbfgs\", \"saga\", \"liblinear\"],  # Included solvers supporting different penalties\n",
        "        # Ensure compatibility between penalty and solver\n",
        "    },\n",
        "    \"Decision Tree\": {\n",
        "        \"max_depth\": [None, 10, 20, 30],\n",
        "        \"min_samples_split\": [2, 5, 10],\n",
        "        \"min_samples_leaf\": [1, 2, 4],  # Added for controlling overfitting\n",
        "        \"criterion\": [\"gini\", \"entropy\"],\n",
        "        \"max_features\": [\"sqrt\", \"log2\", None]  # Controls number of features for splits\n",
        "    },\n",
        "    \"Naive Bayes\": {\n",
        "       #none\n",
        "    },\n",
        "    \"Gradient Boosting\": {\n",
        "       \"n_estimators\": [50, 100, 150],\n",
        "        \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "        \"max_depth\": [3, 5, 7]\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tOx9BeTbFSQ",
        "outputId": "31c70af4-70d1-4746-82d5-58e9d8b5475e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning KNN...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters for KNN: {'metric': 'euclidean', 'n_neighbors': 7, 'weights': 'distance'}\n",
            "                                               params  mean_test_f1  \\\n",
            "21  {'metric': 'minkowski', 'n_neighbors': 7, 'wei...      0.955005   \n",
            "5   {'metric': 'euclidean', 'n_neighbors': 7, 'wei...      0.955005   \n",
            "23  {'metric': 'minkowski', 'n_neighbors': 9, 'wei...      0.954878   \n",
            "7   {'metric': 'euclidean', 'n_neighbors': 9, 'wei...      0.954878   \n",
            "3   {'metric': 'euclidean', 'n_neighbors': 5, 'wei...      0.954732   \n",
            "19  {'metric': 'minkowski', 'n_neighbors': 5, 'wei...      0.954732   \n",
            "1   {'metric': 'euclidean', 'n_neighbors': 3, 'wei...      0.954537   \n",
            "17  {'metric': 'minkowski', 'n_neighbors': 3, 'wei...      0.954537   \n",
            "16  {'metric': 'minkowski', 'n_neighbors': 3, 'wei...      0.953375   \n",
            "0   {'metric': 'euclidean', 'n_neighbors': 3, 'wei...      0.953375   \n",
            "11  {'metric': 'manhattan', 'n_neighbors': 5, 'wei...      0.953197   \n",
            "4   {'metric': 'euclidean', 'n_neighbors': 7, 'wei...      0.953133   \n",
            "20  {'metric': 'minkowski', 'n_neighbors': 7, 'wei...      0.953133   \n",
            "6   {'metric': 'euclidean', 'n_neighbors': 9, 'wei...      0.952962   \n",
            "22  {'metric': 'minkowski', 'n_neighbors': 9, 'wei...      0.952962   \n",
            "9   {'metric': 'manhattan', 'n_neighbors': 3, 'wei...      0.952890   \n",
            "18  {'metric': 'minkowski', 'n_neighbors': 5, 'wei...      0.952845   \n",
            "2   {'metric': 'euclidean', 'n_neighbors': 5, 'wei...      0.952845   \n",
            "15  {'metric': 'manhattan', 'n_neighbors': 9, 'wei...      0.952637   \n",
            "13  {'metric': 'manhattan', 'n_neighbors': 7, 'wei...      0.952540   \n",
            "8   {'metric': 'manhattan', 'n_neighbors': 3, 'wei...      0.951923   \n",
            "10  {'metric': 'manhattan', 'n_neighbors': 5, 'wei...      0.951327   \n",
            "14  {'metric': 'manhattan', 'n_neighbors': 9, 'wei...      0.950867   \n",
            "12  {'metric': 'manhattan', 'n_neighbors': 7, 'wei...      0.950832   \n",
            "\n",
            "    mean_test_precision  mean_test_recall  \n",
            "21             0.964177          0.946008  \n",
            "5              0.964177          0.946008  \n",
            "23             0.965511          0.944477  \n",
            "7              0.965511          0.944477  \n",
            "3              0.962097          0.947480  \n",
            "19             0.962097          0.947480  \n",
            "1              0.958515          0.950600  \n",
            "17             0.958515          0.950600  \n",
            "16             0.957547          0.949246  \n",
            "0              0.957547          0.949246  \n",
            "11             0.962878          0.943712  \n",
            "4              0.962930          0.943535  \n",
            "20             0.962930          0.943535  \n",
            "6              0.963810          0.942357  \n",
            "22             0.963810          0.942357  \n",
            "9              0.959039          0.946832  \n",
            "18             0.960574          0.945243  \n",
            "2              0.960574          0.945243  \n",
            "15             0.965306          0.940297  \n",
            "13             0.965357          0.940061  \n",
            "8              0.958465          0.945478  \n",
            "10             0.961514          0.941357  \n",
            "14             0.963719          0.938354  \n",
            "12             0.964395          0.937647  \n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.51      0.67       813\n",
            "           1       0.54      0.99      0.70       472\n",
            "\n",
            "    accuracy                           0.68      1285\n",
            "   macro avg       0.76      0.75      0.68      1285\n",
            "weighted avg       0.82      0.68      0.68      1285\n",
            "\n",
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "20 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.8983833  0.89831964 0.89693631        nan 0.89223271 0.8875664\n",
            " 0.88298479 0.88291641 0.88272832        nan 0.88155783 0.88153571\n",
            " 0.88026063 0.88007721 0.87996584        nan 0.88014261 0.88015122\n",
            " 0.88022449 0.88023204 0.8802908         nan 0.88023204 0.88023154]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.74175679 0.74175679 0.74193344        nan 0.7399315  0.74175681\n",
            " 0.76324752 0.76318863 0.76312973        nan 0.76377743 0.76360079\n",
            " 0.76825233 0.76819346 0.76825233        nan 0.76825233 0.76831121\n",
            " 0.76884111 0.76889998 0.76889998        nan 0.76889998 0.76889998]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.81258225 0.81255615 0.81209601        nan 0.80896692 0.80812855\n",
            " 0.81875942 0.818696   0.81858111        nan 0.81845045 0.81833909\n",
            " 0.82044984 0.82033644 0.82032166        nan 0.82039856 0.82043581\n",
            " 0.82076978 0.82080683 0.82083242        nan 0.82080683 0.82080668]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters for Logistic Regression: {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "                                               params  mean_test_f1  \\\n",
            "20  {'C': 10, 'penalty': 'l2', 'solver': 'liblinear'}      0.820832   \n",
            "22       {'C': 10, 'penalty': 'l1', 'solver': 'saga'}      0.820807   \n",
            "19       {'C': 10, 'penalty': 'l2', 'solver': 'saga'}      0.820807   \n",
            "23  {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}      0.820807   \n",
            "18      {'C': 10, 'penalty': 'l2', 'solver': 'lbfgs'}      0.820770   \n",
            "12       {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}      0.820450   \n",
            "17   {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}      0.820436   \n",
            "16        {'C': 1, 'penalty': 'l1', 'solver': 'saga'}      0.820399   \n",
            "13        {'C': 1, 'penalty': 'l2', 'solver': 'saga'}      0.820336   \n",
            "14   {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}      0.820322   \n",
            "6      {'C': 0.1, 'penalty': 'l2', 'solver': 'lbfgs'}      0.818759   \n",
            "7       {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}      0.818696   \n",
            "8   {'C': 0.1, 'penalty': 'l2', 'solver': 'libline...      0.818581   \n",
            "10      {'C': 0.1, 'penalty': 'l1', 'solver': 'saga'}      0.818450   \n",
            "11  {'C': 0.1, 'penalty': 'l1', 'solver': 'libline...      0.818339   \n",
            "0     {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}      0.812582   \n",
            "1      {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}      0.812556   \n",
            "2   {'C': 0.01, 'penalty': 'l2', 'solver': 'liblin...      0.812096   \n",
            "4      {'C': 0.01, 'penalty': 'l1', 'solver': 'saga'}      0.808967   \n",
            "5   {'C': 0.01, 'penalty': 'l1', 'solver': 'liblin...      0.808129   \n",
            "3     {'C': 0.01, 'penalty': 'l1', 'solver': 'lbfgs'}           NaN   \n",
            "9      {'C': 0.1, 'penalty': 'l1', 'solver': 'lbfgs'}           NaN   \n",
            "15       {'C': 1, 'penalty': 'l1', 'solver': 'lbfgs'}           NaN   \n",
            "21      {'C': 10, 'penalty': 'l1', 'solver': 'lbfgs'}           NaN   \n",
            "\n",
            "    mean_test_precision  mean_test_recall  \n",
            "20             0.880291          0.768900  \n",
            "22             0.880232          0.768900  \n",
            "19             0.880232          0.768900  \n",
            "23             0.880232          0.768900  \n",
            "18             0.880224          0.768841  \n",
            "12             0.880261          0.768252  \n",
            "17             0.880151          0.768311  \n",
            "16             0.880143          0.768252  \n",
            "13             0.880077          0.768193  \n",
            "14             0.879966          0.768252  \n",
            "6              0.882985          0.763248  \n",
            "7              0.882916          0.763189  \n",
            "8              0.882728          0.763130  \n",
            "10             0.881558          0.763777  \n",
            "11             0.881536          0.763601  \n",
            "0              0.898383          0.741757  \n",
            "1              0.898320          0.741757  \n",
            "2              0.896936          0.741933  \n",
            "4              0.892233          0.739931  \n",
            "5              0.887566          0.741757  \n",
            "3                   NaN               NaN  \n",
            "9                   NaN               NaN  \n",
            "15                  NaN               NaN  \n",
            "21                  NaN               NaN  \n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.94      0.81       813\n",
            "           1       0.77      0.34      0.47       472\n",
            "\n",
            "    accuracy                           0.72      1285\n",
            "   macro avg       0.74      0.64      0.64      1285\n",
            "weighted avg       0.73      0.72      0.69      1285\n",
            "\n",
            "Tuning Decision Tree...\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 20, 'max_features': None, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
            "                                                params  mean_test_f1  \\\n",
            "186  {'criterion': 'entropy', 'max_depth': 20, 'max...      0.933457   \n",
            "187  {'criterion': 'entropy', 'max_depth': 20, 'max...      0.932647   \n",
            "214  {'criterion': 'entropy', 'max_depth': 30, 'max...      0.932534   \n",
            "213  {'criterion': 'entropy', 'max_depth': 30, 'max...      0.932430   \n",
            "132  {'criterion': 'entropy', 'max_depth': None, 'm...      0.932104   \n",
            "..                                                 ...           ...   \n",
            "30   {'criterion': 'gini', 'max_depth': 10, 'max_fe...      0.903928   \n",
            "135  {'criterion': 'entropy', 'max_depth': 10, 'max...      0.903639   \n",
            "137  {'criterion': 'entropy', 'max_depth': 10, 'max...      0.903214   \n",
            "142  {'criterion': 'entropy', 'max_depth': 10, 'max...      0.902839   \n",
            "140  {'criterion': 'entropy', 'max_depth': 10, 'max...      0.896664   \n",
            "\n",
            "     mean_test_precision  mean_test_recall  \n",
            "186             0.942417          0.924694  \n",
            "187             0.941622          0.923870  \n",
            "214             0.941890          0.923399  \n",
            "213             0.941187          0.923870  \n",
            "132             0.941249          0.923164  \n",
            "..                   ...               ...  \n",
            "30              0.919317          0.889249  \n",
            "135             0.903756          0.903732  \n",
            "137             0.909477          0.897197  \n",
            "142             0.920666          0.885893  \n",
            "140             0.913794          0.880830  \n",
            "\n",
            "[216 rows x 4 columns]\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.53      0.68       813\n",
            "           1       0.54      0.95      0.69       472\n",
            "\n",
            "    accuracy                           0.69      1285\n",
            "   macro avg       0.75      0.74      0.69      1285\n",
            "weighted avg       0.80      0.69      0.69      1285\n",
            "\n",
            "Tuning Naive Bayes...\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.92      0.83       813\n",
            "           1       0.78      0.51      0.62       472\n",
            "\n",
            "    accuracy                           0.77      1285\n",
            "   macro avg       0.77      0.72      0.73      1285\n",
            "weighted avg       0.77      0.77      0.76      1285\n",
            "\n",
            "Tuning Gradient Boosting...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 150}\n",
            "                                               params  mean_test_f1  \\\n",
            "26  {'learning_rate': 0.2, 'max_depth': 7, 'n_esti...      0.959440   \n",
            "17  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...      0.958745   \n",
            "25  {'learning_rate': 0.2, 'max_depth': 7, 'n_esti...      0.958321   \n",
            "16  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...      0.957945   \n",
            "23  {'learning_rate': 0.2, 'max_depth': 5, 'n_esti...      0.957287   \n",
            "24  {'learning_rate': 0.2, 'max_depth': 7, 'n_esti...      0.956383   \n",
            "22  {'learning_rate': 0.2, 'max_depth': 5, 'n_esti...      0.955589   \n",
            "14  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...      0.955566   \n",
            "15  {'learning_rate': 0.1, 'max_depth': 7, 'n_esti...      0.952472   \n",
            "21  {'learning_rate': 0.2, 'max_depth': 5, 'n_esti...      0.952334   \n",
            "13  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...      0.951902   \n",
            "20  {'learning_rate': 0.2, 'max_depth': 3, 'n_esti...      0.950869   \n",
            "19  {'learning_rate': 0.2, 'max_depth': 3, 'n_esti...      0.945973   \n",
            "12  {'learning_rate': 0.1, 'max_depth': 5, 'n_esti...      0.941370   \n",
            "11  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...      0.940877   \n",
            "8   {'learning_rate': 0.01, 'max_depth': 7, 'n_est...      0.939209   \n",
            "18  {'learning_rate': 0.2, 'max_depth': 3, 'n_esti...      0.934815   \n",
            "10  {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...      0.934741   \n",
            "7   {'learning_rate': 0.01, 'max_depth': 7, 'n_est...      0.933821   \n",
            "6   {'learning_rate': 0.01, 'max_depth': 7, 'n_est...      0.927534   \n",
            "5   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...      0.922659   \n",
            "9   {'learning_rate': 0.1, 'max_depth': 3, 'n_esti...      0.922101   \n",
            "4   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...      0.916705   \n",
            "3   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...      0.908579   \n",
            "2   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...      0.851230   \n",
            "1   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...      0.847180   \n",
            "0   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...      0.835690   \n",
            "\n",
            "    mean_test_precision  mean_test_recall  \n",
            "26             0.969247          0.949835  \n",
            "17             0.969549          0.948186  \n",
            "25             0.967887          0.948952  \n",
            "16             0.969391          0.946773  \n",
            "23             0.967365          0.947421  \n",
            "24             0.967371          0.945655  \n",
            "22             0.966970          0.944477  \n",
            "14             0.968161          0.943299  \n",
            "15             0.966717          0.938648  \n",
            "21             0.965745          0.939296  \n",
            "13             0.965540          0.938648  \n",
            "20             0.964910          0.937235  \n",
            "19             0.962641          0.929875  \n",
            "12             0.961332          0.922221  \n",
            "11             0.959546          0.922927  \n",
            "8              0.959587          0.919689  \n",
            "18             0.956561          0.914037  \n",
            "10             0.956667          0.913801  \n",
            "7              0.955527          0.913095  \n",
            "6              0.954089          0.902438  \n",
            "5              0.958629          0.889308  \n",
            "9              0.960582          0.886599  \n",
            "4              0.956144          0.880417  \n",
            "3              0.945701          0.874293  \n",
            "2              0.978603          0.753238  \n",
            "1              0.979752          0.746231  \n",
            "0              0.980978          0.728569  \n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.50      0.67       813\n",
            "           1       0.54      0.98      0.69       472\n",
            "\n",
            "    accuracy                           0.68      1285\n",
            "   macro avg       0.76      0.74      0.68      1285\n",
            "weighted avg       0.82      0.68      0.68      1285\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#GRID SEARCH DEFOG TURN\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "best_models = {}\n",
        "scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True)\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "\n",
        "    if model_name in param_grids and param_grids[model_name]:  # Check if there are hyperparameters to tune\n",
        "        # Use GridSearchCV or RandomizedSearchCV\n",
        "        search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[model_name],\n",
        "            scoring='f1_macro',  # Metric to optimize\n",
        "            refit=True,\n",
        "            cv=5,  # Cross-validation folds\n",
        "            verbose=3,  # Print progress\n",
        "            n_jobs=-1  # Use all available cores\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        search.fit(X_train_dturn, y_train_dturn)\n",
        "\n",
        "        # Get the best model and hyperparameters\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        print(f\"Best Parameters for {model_name}: {best_params}\")\n",
        "\n",
        "        # Display sorted results for all metrics\n",
        "        results = pd.DataFrame(search.cv_results_)\n",
        "        print(results[['params', 'mean_test_f1', 'mean_test_precision', 'mean_test_recall']]\n",
        "              .sort_values(by='mean_test_f1', ascending=False))\n",
        "    else:\n",
        "        # If no hyperparameters to tune, use the default model\n",
        "        best_model = model\n",
        "        best_model.fit(X_train_dturn, y_train_dturn)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_dturn = best_model.predict(X_test_dturn)\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_dturn, y_pred_dturn))\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "99mfwE-A2zLT",
        "outputId": "e3c52e4b-a242-4cf7-853b-46840314491d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning KNN...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters for KNN: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      1256\n",
            "           1       0.27      0.10      0.15        29\n",
            "\n",
            "    accuracy                           0.97      1285\n",
            "   macro avg       0.63      0.55      0.57      1285\n",
            "weighted avg       0.96      0.97      0.97      1285\n",
            "\n",
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "20 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.99894577 0.99894577 0.99868702        nan 0.99829089 0.99813603\n",
            " 0.99875768 0.99875768 0.99870618        nan 0.99855182 0.99855182\n",
            " 0.9983977  0.9983977  0.9983977         nan 0.9983977  0.9983977\n",
            " 0.9983977  0.9983977  0.9983977         nan 0.9983977  0.9983977 ]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.9492731  0.9492731  0.9492731         nan 0.94922118 0.9492731\n",
            " 0.9492731  0.9492731  0.9492731         nan 0.94922118 0.9492731\n",
            " 0.9492731  0.9492731  0.9492731         nan 0.9492731  0.9492731\n",
            " 0.9492731  0.9492731  0.9492731         nan 0.9492731  0.9492731 ]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.97043464 0.97043464 0.97030504        nan 0.97006316 0.97001956\n",
            " 0.97033075 0.97033075 0.97030489        nan 0.97019331 0.97022736\n",
            " 0.97014989 0.97014989 0.97014989        nan 0.97014989 0.97014989\n",
            " 0.97014989 0.97014989 0.97014989        nan 0.97014989 0.97014989]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters for Logistic Regression: {'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.98      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.98      0.97      1285\n",
            "\n",
            "Tuning Decision Tree...\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 10}\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.97      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.97      0.96      1285\n",
            "\n",
            "Tuning Naive Bayes...\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.98      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.98      0.97      1285\n",
            "\n",
            "Tuning Gradient Boosting...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters for Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 150}\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.98      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.98      0.97      1285\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "#GRID SEARCH DEFOG WALK\n",
        "best_models = {}\n",
        "scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "\n",
        "    if model_name in param_grids and param_grids[model_name]:  # Check if there are hyperparameters to tune\n",
        "        # Use GridSearchCV or RandomizedSearchCV\n",
        "        search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[model_name],\n",
        "            scoring='f1_macro',  # Metric to optimize\n",
        "            refit=True,\n",
        "            cv=5,  # Cross-validation folds\n",
        "            verbose=3,  # Print progress\n",
        "            n_jobs=-1  # Use all available cores\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        search.fit(X_train_dwalk, y_train_dwalk)\n",
        "\n",
        "        # Get the best model and hyperparameters\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        print(f\"Best Parameters for {model_name}: {best_params}\")\n",
        "    else:\n",
        "        # If no hyperparameters to tune, use the default model\n",
        "        best_model = model\n",
        "        best_model.fit(X_train_dwalk, y_train_dwalk)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_dwalk = best_model.predict(X_test_dwalk)\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_dwalk, y_pred_dwalk))\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YtDtX6Ic29IQ"
      },
      "outputs": [],
      "source": [
        "#GRID SEARCH DEFOG START HESITATION\n",
        "best_models = {}\n",
        "scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "\n",
        "    if model_name in param_grids and param_grids[model_name]:  # Check if there are hyperparameters to tune\n",
        "        # Use GridSearchCV or RandomizedSearchCV\n",
        "        search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[model_name],\n",
        "            scoring=scoring,  # Metric to optimize\n",
        "            refit='f1',\n",
        "            verbose=3,  # Print progress\n",
        "            n_jobs=-1  # Use all available cores\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        search.fit(X_train_dsh, y_train_dsh)\n",
        "\n",
        "        # Get the best model and hyperparameters\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        print(f\"Best Parameters for {model_name}: {best_params}\")\n",
        "    else:\n",
        "        # If no hyperparameters to tune, use the default model\n",
        "        best_model = model\n",
        "        best_model.fit(X_train_dsh, y_train_dsh)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_dsh = best_model.predict(X_test_dsh)\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_dsh, y_pred_dsh))\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJf7MXRX3I-p",
        "outputId": "eb42b8f5-1558-4bb4-ec75-235dceeee743"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning KNN...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters for KNN: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.15      0.26       137\n",
            "           1       0.65      0.98      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.73      0.57      0.52       361\n",
            "weighted avg       0.71      0.66      0.58       361\n",
            "\n",
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "20 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.87843173 0.87843164 0.87852753        nan 0.87749269 0.87827725\n",
            " 0.8799563  0.8800885  0.88006154        nan 0.87996217 0.87998191\n",
            " 0.88049157 0.88046313 0.88048971        nan 0.88043806 0.88043805\n",
            " 0.88046315 0.88049069 0.88049069        nan 0.88049069 0.88049069]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Logistic Regression: {'C': 1, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.12      0.21       137\n",
            "           1       0.65      0.99      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.74      0.55      0.49       361\n",
            "weighted avg       0.72      0.66      0.56       361\n",
            "\n",
            "Tuning Decision Tree...\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.19      0.30       137\n",
            "           1       0.66      0.96      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.69      0.57      0.54       361\n",
            "weighted avg       0.68      0.66      0.60       361\n",
            "\n",
            "Tuning Naive Bayes...\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.08      0.13       137\n",
            "           1       0.62      0.91      0.74       224\n",
            "\n",
            "    accuracy                           0.60       361\n",
            "   macro avg       0.49      0.50      0.43       361\n",
            "weighted avg       0.52      0.60      0.51       361\n",
            "\n",
            "Tuning Gradient Boosting...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Parameters for Gradient Boosting: {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 150}\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.12      0.21       137\n",
            "           1       0.65      0.99      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.74      0.55      0.49       361\n",
            "weighted avg       0.72      0.66      0.56       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#GRID SEARCH TDCSFOG TURN\n",
        "best_models = {}\n",
        "scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "\n",
        "    if model_name in param_grids and param_grids[model_name]:  # Check if there are hyperparameters to tune\n",
        "        # Use GridSearchCV or RandomizedSearchCV\n",
        "        search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[model_name],\n",
        "            scoring='f1_macro',  # Metric to optimize\n",
        "            refit=True,\n",
        "            cv=5,  # Cross-validation folds\n",
        "            verbose=1,  # Print progress\n",
        "            n_jobs=-1  # Use all available cores\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        search.fit(X_train_tturn, y_train_tturn)\n",
        "\n",
        "        # Get the best model and hyperparameters\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        print(f\"Best Parameters for {model_name}: {best_params}\")\n",
        "    else:\n",
        "        # If no hyperparameters to tune, use the default model\n",
        "        best_model = model\n",
        "        best_model.fit(X_train_tturn, y_train_tturn)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_tturn = best_model.predict(X_test_tturn)\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_tturn, y_pred_tturn))\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJIWIPl3JDQ",
        "outputId": "2587e010-0937-4774-8ff9-2f011d7ddbb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning KNN...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters for KNN: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'uniform'}\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.89       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.89      0.85       361\n",
            "\n",
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "20 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.96979929 0.96981698 0.96936484        nan 0.96957456 0.96933016\n",
            " 0.9698417  0.96985665 0.96981463        nan 0.96985309 0.96980561\n",
            " 0.96984777 0.96983283 0.96983283        nan 0.96983283 0.96983283\n",
            " 0.96983283 0.96983283 0.96983283        nan 0.96983283 0.96983283]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters for Logistic Regression: {'C': 0.1, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.89       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.89      0.85       361\n",
            "\n",
            "Tuning Decision Tree...\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.98      0.94       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.89       361\n",
            "   macro avg       0.45      0.49      0.47       361\n",
            "weighted avg       0.81      0.89      0.85       361\n",
            "\n",
            "Tuning Naive Bayes...\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.89       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.89      0.85       361\n",
            "\n",
            "Tuning Gradient Boosting...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Parameters for Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.90       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.90      0.85       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#GRID SEARCH TDCSFOG WALK\n",
        "best_models = {}\n",
        "scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "\n",
        "    if model_name in param_grids and param_grids[model_name]:  # Check if there are hyperparameters to tune\n",
        "        # Use GridSearchCV or RandomizedSearchCV\n",
        "        search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[model_name],\n",
        "            scoring='f1_macro',  # Metric to optimize\n",
        "            refit=True,\n",
        "            cv=5,  # Cross-validation folds\n",
        "            verbose=3,  # Print progress\n",
        "            n_jobs=-1  # Use all available cores\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        search.fit(X_train_twalk, y_train_twalk)\n",
        "\n",
        "        # Get the best model and hyperparameters\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        print(f\"Best Parameters for {model_name}: {best_params}\")\n",
        "    else:\n",
        "        # If no hyperparameters to tune, use the default model\n",
        "        best_model = model\n",
        "        best_model.fit(X_train_twalk, y_train_twalk)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_twalk = best_model.predict(X_test_twalk)\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_twalk, y_pred_twalk))\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y9TnYcX3JLT",
        "outputId": "1ed0a778-266b-4ecd-bc09-d7d7205623f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning KNN...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
            "Best Parameters for KNN: {'metric': 'euclidean', 'n_neighbors': 9, 'weights': 'uniform'}\n",
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.94      0.83       268\n",
            "           1       0.21      0.04      0.07        93\n",
            "\n",
            "    accuracy                           0.71       361\n",
            "   macro avg       0.48      0.49      0.45       361\n",
            "weighted avg       0.60      0.71      0.63       361\n",
            "\n",
            "Tuning Logistic Regression...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:540: FitFailedWarning: \n",
            "20 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 888, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/base.py\", line 1473, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1194, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:1103: UserWarning: One or more of the test scores are non-finite: [0.95081383 0.9508341  0.95075896        nan 0.95047488 0.9502824\n",
            " 0.95068437 0.95068437 0.95071022        nan 0.95067428 0.95065878\n",
            " 0.9506381  0.95065334 0.95065334        nan 0.95063796 0.95065334\n",
            " 0.95062273 0.95063796 0.95063796        nan 0.95063796 0.95063796]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters for Logistic Regression: {'C': 0.01, 'penalty': 'l2', 'solver': 'saga'}\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.97      0.84       268\n",
            "           1       0.00      0.00      0.00        93\n",
            "\n",
            "    accuracy                           0.72       361\n",
            "   macro avg       0.37      0.49      0.42       361\n",
            "weighted avg       0.55      0.72      0.62       361\n",
            "\n",
            "Tuning Decision Tree...\n",
            "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n",
            "Best Parameters for Decision Tree: {'criterion': 'entropy', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.93      0.83       268\n",
            "           1       0.39      0.14      0.21        93\n",
            "\n",
            "    accuracy                           0.72       361\n",
            "   macro avg       0.58      0.53      0.52       361\n",
            "weighted avg       0.66      0.72      0.67       361\n",
            "\n",
            "Tuning Naive Bayes...\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.92      0.82       268\n",
            "           1       0.29      0.10      0.15        93\n",
            "\n",
            "    accuracy                           0.71       361\n",
            "   macro avg       0.52      0.51      0.48       361\n",
            "weighted avg       0.63      0.71      0.65       361\n",
            "\n",
            "Tuning Gradient Boosting...\n",
            "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n",
            "Best Parameters for Gradient Boosting: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      1.00      0.85       268\n",
            "           1       0.00      0.00      0.00        93\n",
            "\n",
            "    accuracy                           0.74       361\n",
            "   macro avg       0.37      0.50      0.43       361\n",
            "weighted avg       0.55      0.74      0.63       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#GRID SEARCH TDCSFOG START HESITATION\n",
        "best_models = {}\n",
        "scoring = {'precision': 'precision', 'recall': 'recall', 'f1': 'f1'}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Tuning {model_name}...\")\n",
        "\n",
        "    if model_name in param_grids and param_grids[model_name]:  # Check if there are hyperparameters to tune\n",
        "        # Use GridSearchCV or RandomizedSearchCV\n",
        "        search = GridSearchCV(\n",
        "            estimator=model,\n",
        "            param_grid=param_grids[model_name],\n",
        "            scoring='f1_macro',  # Metric to optimize\n",
        "            refit=True,\n",
        "            cv=5,  # Cross-validation folds\n",
        "            verbose=3,  # Print progress\n",
        "            n_jobs=-1  # Use all available cores\n",
        "        )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        search.fit(X_train_tsh, y_train_tsh)\n",
        "\n",
        "        # Get the best model and hyperparameters\n",
        "        best_model = search.best_estimator_\n",
        "        best_params = search.best_params_\n",
        "        print(f\"Best Parameters for {model_name}: {best_params}\")\n",
        "    else:\n",
        "        # If no hyperparameters to tune, use the default model\n",
        "        best_model = model\n",
        "        best_model.fit(X_train_tsh, y_train_tsh)\n",
        "\n",
        "    # Evaluate on test data\n",
        "    y_pred_tsh = best_model.predict(X_test_tsh)\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_tsh, y_pred_tsh))\n",
        "\n",
        "    # Store the best model\n",
        "    best_models[model_name] = best_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5s3aponsfKs"
      },
      "outputs": [],
      "source": [
        "models_dturn = {\n",
        "    \"KNN\": KNeighborsClassifier(metric='euclidean', n_neighbors=7, weights='distance'),\n",
        "    \"Logistic Regression\": LogisticRegression(C=10, penalty= 'l2', solver='liblinear', class_weight='balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(criterion= 'entropy', max_depth=20, max_features=None, min_samples_leaf= 4, min_samples_split= 2, class_weight='balanced'),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.2, max_depth= 7, n_estimators=150)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkMNnvHvwu-1",
        "outputId": "fed2ee8f-b6aa-49e8-d23e-066fa2b43171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.51      0.67       813\n",
            "           1       0.54      0.99      0.70       472\n",
            "\n",
            "    accuracy                           0.68      1285\n",
            "   macro avg       0.76      0.75      0.68      1285\n",
            "weighted avg       0.82      0.68      0.68      1285\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.93      0.83       813\n",
            "           1       0.79      0.45      0.57       472\n",
            "\n",
            "    accuracy                           0.75      1285\n",
            "   macro avg       0.77      0.69      0.70      1285\n",
            "weighted avg       0.76      0.75      0.73      1285\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.53      0.69       813\n",
            "           1       0.54      0.96      0.70       472\n",
            "\n",
            "    accuracy                           0.69      1285\n",
            "   macro avg       0.75      0.75      0.69      1285\n",
            "weighted avg       0.81      0.69      0.69      1285\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.92      0.83       813\n",
            "           1       0.78      0.51      0.62       472\n",
            "\n",
            "    accuracy                           0.77      1285\n",
            "   macro avg       0.77      0.72      0.73      1285\n",
            "weighted avg       0.77      0.77      0.76      1285\n",
            "\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.50      0.66       813\n",
            "           1       0.53      0.98      0.69       472\n",
            "\n",
            "    accuracy                           0.68      1285\n",
            "   macro avg       0.76      0.74      0.68      1285\n",
            "weighted avg       0.82      0.68      0.67      1285\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TRAIN DEFOG -> TURN\n",
        "for model_name, model in models_dturn.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_dturn, y_train_dturn)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_dturn = model.predict(X_test_dturn)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_dturn, y_pred_dturn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4e4WZubHwJ-"
      },
      "outputs": [],
      "source": [
        "models_dwalk = {\n",
        "    \"KNN\": KNeighborsClassifier(metric='manhattan', n_neighbors=9, weights='uniform'),\n",
        "    \"Logistic Regression\": LogisticRegression(C=.01, penalty= 'l2', solver='lbfgs', class_weight = 'balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(criterion= 'entropy', max_depth=10, max_features='sqrt', min_samples_leaf=2, min_samples_split=2, class_weight = 'balanced'),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.1, max_depth= 5, n_estimators=150)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y9a6Tt8bFfB",
        "outputId": "9df5a730-d2bd-4e10-94bd-cdf6da44077f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      1256\n",
            "           1       0.27      0.10      0.15        29\n",
            "\n",
            "    accuracy                           0.97      1285\n",
            "   macro avg       0.63      0.55      0.57      1285\n",
            "weighted avg       0.96      0.97      0.97      1285\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.98      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.98      0.97      1285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98      1256\n",
            "           1       0.07      0.07      0.07        29\n",
            "\n",
            "    accuracy                           0.96      1285\n",
            "   macro avg       0.52      0.52      0.52      1285\n",
            "weighted avg       0.96      0.96      0.96      1285\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.98      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.98      0.97      1285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1256\n",
            "           1       0.00      0.00      0.00        29\n",
            "\n",
            "    accuracy                           0.98      1285\n",
            "   macro avg       0.49      0.50      0.49      1285\n",
            "weighted avg       0.96      0.98      0.97      1285\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TRAIN DEFOG -> WALKING\n",
        "for model_name, model in models_dwalk.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_dwalk, y_train_dwalk)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_dwalk = model.predict(X_test_dwalk)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_dwalk, y_pred_dwalk))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91JxnRVEHuSR"
      },
      "outputs": [],
      "source": [
        "models_dsh = {\n",
        "    \"KNN\": KNeighborsClassifier(metric = 'euclidean', n_neighbors=3, weights='uniform'),\n",
        "    \"Logistic Regression\": LogisticRegression(C=1, penalty= 'l2', solver='lbfgs', class_weight = 'balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(criterion= 'entropy', max_depth=None, min_samples_split=10,  class_weight = 'balanced'),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.01, max_depth= 3, n_estimators=50)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf4J51lFx-4u",
        "outputId": "e2cff077-cf05-4a73-b6d5-fdd53e4ef450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1285\n",
            "\n",
            "    accuracy                           1.00      1285\n",
            "   macro avg       1.00      1.00      1.00      1285\n",
            "weighted avg       1.00      1.00      1.00      1285\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1285\n",
            "\n",
            "    accuracy                           1.00      1285\n",
            "   macro avg       1.00      1.00      1.00      1285\n",
            "weighted avg       1.00      1.00      1.00      1285\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1285\n",
            "           1       0.00      0.00      0.00         0\n",
            "\n",
            "    accuracy                           1.00      1285\n",
            "   macro avg       0.50      0.50      0.50      1285\n",
            "weighted avg       1.00      1.00      1.00      1285\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1285\n",
            "\n",
            "    accuracy                           1.00      1285\n",
            "   macro avg       1.00      1.00      1.00      1285\n",
            "weighted avg       1.00      1.00      1.00      1285\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1285\n",
            "\n",
            "    accuracy                           1.00      1285\n",
            "   macro avg       1.00      1.00      1.00      1285\n",
            "weighted avg       1.00      1.00      1.00      1285\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TRAIN DEFOG -> Start Hesitation\n",
        "for model_name, model in models_dsh.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_dsh, y_train_dsh)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_dsh = model.predict(X_test_dsh)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_dsh, y_pred_dsh))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_tturn = {\n",
        "    \"KNN\": KNeighborsClassifier(metric= 'manhattan', n_neighbors=9, weights='distance'),\n",
        "    \"Logistic Regression\": LogisticRegression(C=1, penalty= 'l2', solver='lbfgs', class_weight = 'balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(criterion='entropy', max_depth= 20, max_features= None, min_samples_leaf= 1, min_samples_split= 2, class_weight = 'balanced'),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.2, max_depth= 7, n_estimators=150)\n",
        "}"
      ],
      "metadata": {
        "id": "XQHBB_HWCeIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AW8Y0LNKVifT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60328c6f-0f8e-4e8b-d343-663f372b9614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.15      0.26       137\n",
            "           1       0.65      0.98      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.73      0.57      0.52       361\n",
            "weighted avg       0.71      0.66      0.58       361\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.13      0.23       137\n",
            "           1       0.65      0.99      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.75      0.56      0.51       361\n",
            "weighted avg       0.73      0.66      0.57       361\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.23      0.36       137\n",
            "           1       0.67      0.96      0.79       224\n",
            "\n",
            "    accuracy                           0.68       361\n",
            "   macro avg       0.72      0.59      0.57       361\n",
            "weighted avg       0.71      0.68      0.62       361\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.35      0.08      0.13       137\n",
            "           1       0.62      0.91      0.74       224\n",
            "\n",
            "    accuracy                           0.60       361\n",
            "   macro avg       0.49      0.50      0.43       361\n",
            "weighted avg       0.52      0.60      0.51       361\n",
            "\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.12      0.22       137\n",
            "           1       0.65      0.99      0.78       224\n",
            "\n",
            "    accuracy                           0.66       361\n",
            "   macro avg       0.75      0.56      0.50       361\n",
            "weighted avg       0.72      0.66      0.57       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TRAIN TDCSFOG -> TURN\n",
        "for model_name, model in models_tturn.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_tturn, y_train_tturn)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_tturn = model.predict(X_test_tturn)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_tturn, y_pred_tturn))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_twalk = {\n",
        "    \"KNN\": KNeighborsClassifier(metric= 'manhattan', n_neighbors=9, weights='uniform'),\n",
        "    \"Logistic Regression\": LogisticRegression(C=.1, penalty= 'l2', solver='saga', class_weight = 'balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(criterion='entropy', max_depth= 10, max_features= 'log2', min_samples_leaf= 2, min_samples_split= 2, class_weight = 'balanced'),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.01, max_depth= 5, n_estimators=50)\n",
        "}"
      ],
      "metadata": {
        "id": "wy4gDzVmCe7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrsOYNusyYXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6a3daba-470a-4ba6-ee07-643bfdebeb96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.89       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.89      0.85       361\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.93       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.88       361\n",
            "   macro avg       0.45      0.49      0.47       361\n",
            "weighted avg       0.81      0.88      0.84       361\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.97      0.93       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.88       361\n",
            "   macro avg       0.45      0.49      0.47       361\n",
            "weighted avg       0.81      0.88      0.84       361\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.99      0.94       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.89       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.89      0.85       361\n",
            "\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      1.00      0.95       325\n",
            "           1       0.00      0.00      0.00        36\n",
            "\n",
            "    accuracy                           0.90       361\n",
            "   macro avg       0.45      0.50      0.47       361\n",
            "weighted avg       0.81      0.90      0.85       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TRAIN TDCSFOG -> WALKING\n",
        "for model_name, model in models_twalk.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_twalk, y_train_twalk)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_twalk = model.predict(X_test_twalk)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_twalk, y_pred_twalk))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_tsh = {\n",
        "    \"KNN\": KNeighborsClassifier(metric= 'euclidean', n_neighbors=9, weights='uniform'),\n",
        "    \"Logistic Regression\": LogisticRegression(C=.01, penalty= 'l2', solver='saga', class_weight = 'balanced'),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(criterion='entropy', max_depth= 10, max_features= 'sqrt', min_samples_leaf= 2, min_samples_split= 5, class_weight = 'balanced'),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier(learning_rate=0.01, max_depth= 5, n_estimators=50)\n",
        "}"
      ],
      "metadata": {
        "id": "H9QKrfHKCffc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M71SAcVyyYlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de11183b-4215-4119-ff2d-70583fda6c83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.94      0.83       268\n",
            "           1       0.21      0.04      0.07        93\n",
            "\n",
            "    accuracy                           0.71       361\n",
            "   macro avg       0.48      0.49      0.45       361\n",
            "weighted avg       0.60      0.71      0.63       361\n",
            "\n",
            "Logistic Regression Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.93      0.82       268\n",
            "           1       0.05      0.01      0.02        93\n",
            "\n",
            "    accuracy                           0.69       361\n",
            "   macro avg       0.39      0.47      0.42       361\n",
            "weighted avg       0.55      0.69      0.61       361\n",
            "\n",
            "Decision Tree Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.89      0.82       268\n",
            "           1       0.41      0.22      0.28        93\n",
            "\n",
            "    accuracy                           0.72       361\n",
            "   macro avg       0.59      0.55      0.55       361\n",
            "weighted avg       0.67      0.72      0.68       361\n",
            "\n",
            "Naive Bayes Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.92      0.82       268\n",
            "           1       0.29      0.10      0.15        93\n",
            "\n",
            "    accuracy                           0.71       361\n",
            "   macro avg       0.52      0.51      0.48       361\n",
            "weighted avg       0.63      0.71      0.65       361\n",
            "\n",
            "Gradient Boosting Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      1.00      0.85       268\n",
            "           1       0.00      0.00      0.00        93\n",
            "\n",
            "    accuracy                           0.74       361\n",
            "   macro avg       0.37      0.50      0.43       361\n",
            "weighted avg       0.55      0.74      0.63       361\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#TRAIN TDCSFOG -> Start Hesitation\n",
        "for model_name, model in models_tsh.items():\n",
        "    # Train the model\n",
        "    model.fit(X_train_tsh, y_train_tsh)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred_tsh = model.predict(X_test_tsh)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(f\"{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test_tsh, y_pred_tsh))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iC-Qws8DESeG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}